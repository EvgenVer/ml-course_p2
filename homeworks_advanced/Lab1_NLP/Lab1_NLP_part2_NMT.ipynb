{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztcPpl6hPsAU"
   },
   "source": [
    "# Lab 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6raI4E_fPsAW"
   },
   "source": [
    "## Part 2: Neural Machine Translation in the wild\n",
    "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
    "\n",
    "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
    "\n",
    "Your ultimate task is to use the techniques we've covered, e.g.\n",
    "\n",
    "* Optimization enhancements (e.g. learning rate decay)\n",
    "\n",
    "* CNN encoder (with or without positional encoding)\n",
    "\n",
    "* attention/self-attention mechanism\n",
    "\n",
    "* pretraining the language model\n",
    "\n",
    "* [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt)\n",
    "\n",
    "* or just fine-tunning BERT ;)\n",
    "\n",
    "to improve the translation quality. \n",
    "\n",
    "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
    "\n",
    "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORBsRFA-PsAY",
    "outputId": "cfc978ea-f016-4950-b9a5-01583f74f6d1"
   },
   "outputs": [],
   "source": [
    "# Thanks to YSDA NLP course team for the data\n",
    "# (who thanks tilda and deephack teams for the data in their turn)\n",
    "!wget -nc https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/datasets/Machine_translation_EN_RU/data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ihZ_nVUmsyC"
   },
   "source": [
    "The `data.txt` is a tsv file, each line of which contains a sentence in english and a corresponding translation, separated by `\\t`. We'll load it into memory and create a list of pairs, which would yield the same interface as with the torchtext's datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wzRffwfgizj",
    "outputId": "9bd24e5e-3423-4c78-a8af-710352a180a0"
   },
   "outputs": [],
   "source": [
    "with open(\"data.txt\") as f:\n",
    "    data = [l.rstrip().split(\"\\t\") for l in f]\n",
    "\n",
    "print(f\"Dataset size {len(data):,}\")\n",
    "print(\"Sample:\")\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjDuhDOWPsAa"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RW41CAd1ofXD"
   },
   "source": [
    "First of all, let's split our dataset into train, test and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXYV0vWaolpm",
    "outputId": "1dea33f1-ecd0-4fd2-ee3e-de2a30475fb4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "data_size = len(data)\n",
    "train_size = int(0.8 * data_size)\n",
    "test_size = int(0.15 * data_size)\n",
    "val_size = data_size - train_size - test_size\n",
    "train_data, test_data, val_data = random_split(\n",
    "    data, [train_size, test_size, val_size], generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "print(f\"Val size: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gGXkxD5odoA"
   },
   "source": [
    "Here comes the preprocessing. If you find pieces, that you don't understand, please, go back to 3rd week's practice notebook. The code is mostly taken from it.\n",
    "\n",
    "Do not hesitate to use BPE or more complex preprocessing pipeline ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6sGsE2moPsAa"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    return tokenizer.tokenize(sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmshoKhRpQVj"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "from torchtext.vocab import vocab as Vocab\n",
    "\n",
    "\n",
    "src_counter = Counter()\n",
    "trg_counter = Counter()\n",
    "for src, trg in train_data:\n",
    "    src_counter.update(tokenize(src))\n",
    "    trg_counter.update(tokenize(trg))\n",
    "\n",
    "src_vocab = Vocab(src_counter, min_freq=3)\n",
    "trg_vocab = Vocab(trg_counter, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxBsel4JpaTP",
    "outputId": "f12294fb-89e6-471d-b532-65f0a45bff19"
   },
   "outputs": [],
   "source": [
    "unk_token = \"<unk>\"\n",
    "sos_token, eos_token, pad_token = \"<sos>\", \"<eos>\", \"<pad>\"\n",
    "specials = [sos_token, eos_token, pad_token]\n",
    "\n",
    "for vocab in [src_vocab, trg_vocab]:\n",
    "    if unk_token not in vocab:\n",
    "        vocab.insert_token(unk_token, index=0)\n",
    "        vocab.set_default_index(0)\n",
    "\n",
    "    for token in specials:\n",
    "        if token not in vocab:\n",
    "            vocab.append_token(token)\n",
    "\n",
    "print(f\"Source (en) vocabulary size: {len(src_vocab)}\")\n",
    "print(f\"Target (ru) vocabulary size: {len(trg_vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpFjrwNHpiZq"
   },
   "outputs": [],
   "source": [
    "def encode(sent, vocab):\n",
    "    tokenized = [sos_token] + tokenize(sent) + [eos_token]\n",
    "    return [vocab[tok] for tok in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QpWUg9RuqGXm",
    "outputId": "5141a0fa-0b41-481c-a046-2e3808d3954f"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    src_list, trg_list = [], []\n",
    "    for src, trg in batch:\n",
    "        src_encoded = encode(src, src_vocab)\n",
    "        src_list.append(torch.tensor(src_encoded))\n",
    "\n",
    "        trg_encoded = encode(trg, trg_vocab)\n",
    "        trg_list.append(torch.tensor(trg_encoded))\n",
    "\n",
    "    src_padded = pad_sequence(src_list, padding_value=src_vocab[pad_token])\n",
    "    trg_padded = pad_sequence(trg_list, padding_value=trg_vocab[pad_token])\n",
    "    return src_padded, trg_padded\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "src_batch, trg_batch = next(iter(train_dataloader))\n",
    "src_batch.shape, trg_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nvNQhzfPsAe"
   },
   "source": [
    "## Model side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKXxEXwFtMPP"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_tokens, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_tokens = n_tokens\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_tokens, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_tokens = n_tokens\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hid_dim, n_tokens)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(dim=0)\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        pred = self.out(output.squeeze(dim=0))\n",
    "        return pred, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \"encoder and decoder must have same hidden dim\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "        ), \"encoder and decoder must have equal number of layers\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        preds = []\n",
    "        hidden = self.encoder(src)\n",
    "\n",
    "        # First input to the decoder is the <sos> token.\n",
    "        input = trg[0, :]\n",
    "        for i in range(1, trg_len):\n",
    "            pred, hidden = self.decoder(input, hidden)\n",
    "            preds.append(pred)\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            _, top_pred = pred.max(dim=1)\n",
    "            input = trg[i, :] if teacher_force else top_pred\n",
    "\n",
    "        return torch.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQ3w4pdDPsAf"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "enc = Encoder(len(src_vocab), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.5)\n",
    "dec = Decoder(len(trg_vocab), emb_dim=256, hid_dim=512, n_layers=2, dropout=0.5)\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_-hFxcyPsAf"
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param, -0.08, 0.08)\n",
    "\n",
    "\n",
    "model.apply(init_weights);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15zD2R7cPsAg",
    "outputId": "eb442561-a498-4aa1-ff79-a5722c7ddbd8"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cG45OrPDPsAg"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab[pad_token])\n",
    "loss_history, train_loss_history, val_loss_history = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "sFsx5PcCudiG",
    "outputId": "66fd1129-a5b7-41a0-c85f-1dc5a440889c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "# Please don't use tensorboard here.\n",
    "# It doesn't save the training plots in the notebook.\n",
    "n_epochs = 50\n",
    "clip = 1\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for src, trg in train_dataloader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        output = model(src, trg)\n",
    "\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if len(loss_history) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            plt.figure(figsize=(15, 5))\n",
    "\n",
    "            plt.subplot(121)\n",
    "            plt.plot(loss_history)\n",
    "            plt.xlabel(\"step\")\n",
    "\n",
    "            plt.subplot(122)\n",
    "            plt.plot(train_loss_history, label=\"train loss\")\n",
    "            plt.plot(val_loss_history, label=\"val loss\")\n",
    "            plt.xlabel(\"epoch\")\n",
    "            plt.legend()\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in val_dataloader:\n",
    "            src, trg = src.to(device), trg.to(device)\n",
    "            output = model(src, trg)\n",
    "\n",
    "            output = output.view(-1, output.shape[-1])\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_loss_history.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp3-T3dDPsAg"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JtYccOGpPsAh",
    "outputId": "16782608-5511-43dc-af8d-546cc2fe01ea"
   },
   "outputs": [],
   "source": [
    "trg_itos = trg_vocab.get_itos()\n",
    "model.eval()\n",
    "max_len = 50\n",
    "with torch.no_grad():\n",
    "    for i, (src, trg) in enumerate(val_data):\n",
    "        encoded = encode(src, src_vocab)[::-1]\n",
    "        encoded = torch.tensor(encoded)[:, None].to(device)\n",
    "        hidden = model.encoder(encoded)\n",
    "\n",
    "        pred_tokens = [trg_vocab[sos_token]]\n",
    "        for _ in range(max_len):\n",
    "            decoder_input = torch.tensor([pred_tokens[-1]]).to(device)\n",
    "            pred, hidden = model.decoder(decoder_input, hidden)\n",
    "            _, pred_token = pred.max(dim=1)\n",
    "            if pred_token == trg_vocab[eos_token]:\n",
    "                # Don't add it to prediction for cleaner output.\n",
    "                break\n",
    "\n",
    "            pred_tokens.append(pred_token.item())\n",
    "\n",
    "        print(f\"src: '{src.rstrip().lower()}'\")\n",
    "        print(f\"trg: '{trg.rstrip().lower()}'\")\n",
    "        print(f\"pred: '{' '.join(trg_itos[i] for i in pred_tokens[1:])}'\")\n",
    "        print()\n",
    "\n",
    "        if i == 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJsWrg5Z1phB"
   },
   "source": [
    "The metric often used in NMT is the BLEU. We'll also use it to evaluate our models. In fact, the goal of this homework is to beat the specified baseline BLEU scores.\n",
    "\n",
    "Here is how you can calculate the score for your model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hogEKcmOPsAh",
    "outputId": "93cf0f94-7f8d-4e13-dcae-a7ed5319b7e3"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "references, hypotheses = [], []\n",
    "with torch.no_grad():\n",
    "    for src, trg in test_dataloader:\n",
    "        output = model(src.to(device), trg.to(device), teacher_forcing_ratio=0)\n",
    "        output = output.cpu().numpy().argmax(axis=2)\n",
    "\n",
    "        for i in range(trg.shape[1]):\n",
    "            reference = trg[:, i]\n",
    "            reference_tokens = [trg_itos[id_] for id_ in reference]\n",
    "            reference_tokens = [tok for tok in reference_tokens if tok not in specials]\n",
    "            references.append(reference_tokens)\n",
    "\n",
    "            hypothesis = output[:, i]\n",
    "            hypothesis_tokens = [trg_itos[id_] for id_ in hypothesis]\n",
    "            hypothesis_tokens = [tok for tok in hypothesis_tokens if tok not in specials]\n",
    "            hypotheses.append(hypothesis_tokens)\n",
    "\n",
    "# corpus_bleu works with multiple references\n",
    "bleu = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "print(f\"Your model shows test BLEU of {100 * bleu:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9hHhduLPsAh"
   },
   "source": [
    "Baseline solution BLEU score is quite low. Try to achieve at least __24__ BLEU on the test set. \n",
    "The checkpoints are:\n",
    "\n",
    "* __22__ - minimal score to submit the homework, 30% of points\n",
    "\n",
    "* __27__ - good score, 70% of points\n",
    "\n",
    "* __29__ - excellent score, 100% of points"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Lab1_NLP_part2_NMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
